<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reinforcement Learning for UAV Obstacle Avoidance | Dongxuan He </title> <meta name="author" content="Dongxuan He"> <meta name="description" content="Learning a reactive UAV controller in a 2D partially observable environment with randomized obstacle layouts"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dongxuanhe.github.io/projects/2_project/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Dongxuan He </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning for UAV Obstacle Avoidance</h1> <p class="post-description">Learning a reactive UAV controller in a 2D partially observable environment with randomized obstacle layouts</p> </header> <article> <h3 id="overview">Overview</h3> <p>This project investigates a <strong>model-free deep reinforcement learning</strong> approach for*UAV obstacle avoidance under partial observability.<br> A continuous-control policy is trained to navigate from a start location to a goal while avoiding randomized static obstacles in a lightweight 2D simulation.</p> <p>Rather than focusing on algorithmic novelty, this work emphasizes <strong>end-to-end problem formulation</strong>, fair baseline comparison, and systematic evaluation as a first research-style RL project.</p> <h3 id="key-topics">Key Topics</h3> <ul> <li>Reactive obstacle avoidance under partial observability (POMDP)</li> <li>Domain randomization across start–goal pairs and obstacle layouts</li> <li>Reward shaping for safe, smooth, and energy-efficient control</li> <li>PPO-based continuous control (acceleration commands)</li> <li>Baseline comparison against a classical Artificial Potential Field controller</li> </ul> <h3 id="problem-motivation">Problem Motivation</h3> <p>Classical autonomy pipelines often combine global planners (e.g., A*/RRT) with tracking controllers (PID/MPC). They can work well when maps are accurate and environments are static, but performance degrades when obstacle layouts vary across missions or global maps are unavailable.</p> <p><strong>Reinforcement learning</strong> offers a model-free alternative: the UAV directly learns a reactive policy that maps local sensing to control actions, enabling navigation without explicit path planning.</p> <h3 id="problem-formulation">Problem Formulation</h3> <p>We formulate the task as a partially observable Markov Decision Process <strong>(POMDP)</strong>.</p> <p><strong>Dynamics</strong> (2D double integrator): The UAV state evolves with a simple inertial model using continuous acceleration commands, with velocity and actions clipped for feasibility.</p> <p><strong>Observations</strong> (local sensing only):</p> <ul> <li>Goal-relative position: \((x_g - x,\; y_g - y)\)</li> <li>Current velocity: \((v_x,\; v_y)\)</li> <li>Ray-based range sensor distances: $N$ rays over a 180° field of view</li> </ul> <p>No global map is provided.</p> <p>Actions (continuous control):</p> <ul> <li>Planar acceleration command: $(a_x, a_y)$</li> </ul> <p><strong>Reward Design</strong></p> <p>The shaped reward encourages safe, smooth, and efficient goal-directed behavior.<br> At each timestep $t$, the total reward is defined as:</p> \[r_t = r_t^{\text{prog}} + r_t^{\text{goal}} + r_t^{\text{col}} + r_t^{\text{safe}} + r_t^{\text{energy}} + r_t^{\text{smooth}} + r_t^{\text{time}}\] <p>Each component is defined as follows.</p> <ul> <li> <p>Goal progress: \(r_t^{\text{prog}} = \alpha \left( \lVert g_t \rVert - \lVert g_{t+1} \rVert \right),\) which rewards reduction in distance to the goal.</p> </li> <li> <p>Goal-reaching bonus: \(r_t^{\text{goal}} = \kappa \, \mathbb{I}\!\left[\lVert g_{t+1} \rVert \le r_{\text{goal}} \right],\) where $\mathbb{I}[\cdot]$ is the indicator function.</p> </li> <li> <p>Collision penalty: \(r_t^{\text{col}} = -\beta \, \mathbb{I}[\text{collision}],\) which strongly penalizes unsafe trajectories.</p> </li> <li> <p>Safety-margin penalty: \(r_t^{\text{safe}} = -\beta_m \, \mathbb{I}\!\left[ \min_i \lVert x_t - c_i \rVert \le r_i + m \right],\) penalizing proximity to obstacles within a safety margin $m$.</p> </li> <li> <p>Energy regularization: \(r_t^{\text{energy}} = -\lambda \lVert a_t \rVert^2,\) discouraging excessive control effort.</p> </li> <li> <p>Smoothness (jerk) regularization: \(r_t^{\text{smooth}} = -\mu \lVert a_t - a_{t-1} \rVert^2,\) penalizing abrupt changes in control.</p> </li> <li> <p>Time penalty: \(r_t^{\text{time}} = -\eta,\) encouraging efficient task completion.</p> </li> </ul> <h3 id="methods">Methods</h3> <p><strong>PPO (learned policy).</strong> We train a continuous-control policy using Proximal Policy Optimization (PPO) with Stable-Baselines3. Training runs for 300k timesteps, and each episode randomizes the start–goal pair and obstacle configuration (domain randomization).</p> <p><strong>APF (classical baseline):</strong> We implement an Artificial Potential Field (APF) controller with goal attraction and obstacle repulsion within an influence radius. APF is reactive and does not require training, but is known to suffer from local minima in cluttered environments.</p> <p>All controllers run under the same dynamics, sensing, and collision checking for a fair comparison.</p> <h3 id="evaluation">Evaluation</h3> <p>Controllers are evaluated on unseen randomized layouts using:</p> <ul> <li>Success rate (reach goal without collision)</li> <li>Collision rate</li> <li>Path length</li> <li>Episode time</li> <li>Energy per step (average squared acceleration)</li> <li>Smoothness per step (average squared change in acceleration / jerk proxy)</li> </ul> <h3 id="results-summary">Results Summary</h3> <p>On randomized test environments, the learned PPO policy achieves substantially higher reliability and better control quality than APF:</p> <ul> <li>Success rate: PPO 0.76 vs. APF 0.00</li> <li>Collision rate: PPO 0.22 vs. APF 0.39</li> <li>Avg path length (m): PPO 5.02 vs. APF 4.75</li> <li>Energy per step: PPO 1.06 vs. APF 8.91</li> <li>Smoothness (jerk/step): PPO 0.10 vs. APF 1.25</li> </ul> <figure class="figure text-center"> <img src="/assets/img/uav/learning_curve_success.png" class="figure-img img-fluid rounded d-block mx-auto" style="max-width: 700px; width: 100%;" alt="Learning curve: success rate vs timesteps"> <figcaption class="figure-caption"> Learning curve showing success rate as a function of training timesteps for the PPO policy. </figcaption> </figure> <p class="mt-2"> The learning curve indicates stable convergence after approximately <strong>100k timesteps</strong>, suggesting that the policy consistently improves under domain randomization. </p> <div class="mt-4"></div> <div class="row"> <div class="col-md-6 text-center"> <img src="/assets/img/uav/fig_success_rate.png" class="img-fluid rounded" style="max-width: 360px; width: 100%;" alt="Final success rate comparison"> <p class="text-muted small mt-2"> Final success rate on unseen randomized environments (PPO vs APF). </p> </div> <div class="col-md-6 text-center"> <img src="/assets/img/uav/fig_energy.png" class="img-fluid rounded" style="max-width: 360px; width: 100%;" alt="Energy per step comparison"> <p class="text-muted small mt-2"> Average energy consumption per step (PPO vs APF). </p> </div> </div> <p>Overall, PPO generalizes to unseen static obstacle layouts and produces trajectories that are safer, smoother, and more energy-efficient, while APF frequently fails in clutter due to local minima.</p> <h3 id="limitations-and-future-work">Limitations and Future Work</h3> <p>This project is limited to a <strong>2D</strong> setting with static obstacles and a restricted <strong>180° range-sensor</strong> observation. Performance degrades in densely cluttered or narrow environments, and reward tuning remains sensitive.</p> <p>Future directions include:</p> <ul> <li>Extending to 3D UAV dynamics</li> <li>Adding temporal memory (e.g., recurrent policies) to mitigate partial observability</li> <li>Handling dynamic obstacles and sensor noise</li> <li>Evaluating in higher-fidelity simulators and exploring sim-to-real transfer</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dongxuan He. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>